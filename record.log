# 2024-8-25
测试发现，batch size64对影响很小 去掉一个dis的有一些作用，尤其是在高信噪比，这也说明原来的思路有一点点作用，参考论文，做出如下改动,
1.train范围：0-20
2.dis改为一个，只保留最小的out，去掉out2，out4
3.超参数从12改为20，从而增强逐像素监督的效果。
4.把lr schedule的bar改为300，提前介入

#2024-8-26（version3）
效果变得非常差，主要的原因考虑：
1.train范围：应该每影响
2.dis，应该有一定影响，但是不大，考虑该回两个
3.超参数，这个应该影响比较大，考虑改为15


#2024-8-28（version4）
效果变得比较好，看来是训练SNR的问题，考虑继续调参数优化
1.改为三个dis，测试范围
2.超参数不变，控制变量

#2024-8-30（version5）
效果变得稍微变差，这证明dis应该越少越好，同时也证明可能目前系统的性能和generator 的 pipeline有关，如果这部分不改可能无法突破autoencoder单独的上限。
1.改为1个dis
2.超参数现在是15，从15改到18看看效果，不知道是越高越好，还是越低越好，但是逻辑上来考虑的话，应该是越低越好，因为首先之前改为20，直接loss飘了，其次这个值太大的话，dis就没用了，不过打算先试试18
3.还有一个观察到的点， 似乎可以把300再改小一点（未改动！）


#2024-8-30(version6)
效果变得很差，看来不能只是使用单独的dis
1.改为2个dis(out和out4）
2.保持18，现在有15和18两个，看看效果

#2024-9-2
效果基本不变，这也应证了一个思路，即如果不改变ae的pipeline，可能效果不会出现大的变化，现在要做的实验主要有：1.验证更大的hpara还是更小的hpara（预计周三完成）2.验证dis是不是好用。
9-2:10:38,验证完毕，鉴别器不好用，考虑可能的原因：就像之前的，损失函数不变
1.dis增加，增强鉴别能力
2.hpara改为25，看看效果
3.先验证vadility（不行）